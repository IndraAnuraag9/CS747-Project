{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e58329dd",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-06T22:31:28.294259Z",
     "iopub.status.busy": "2024-12-06T22:31:28.294022Z",
     "iopub.status.idle": "2024-12-06T22:31:31.208628Z",
     "shell.execute_reply": "2024-12-06T22:31:31.207775Z"
    },
    "papermill": {
     "duration": 2.920664,
     "end_time": "2024-12-06T22:31:31.210682",
     "exception": false,
     "start_time": "2024-12-06T22:31:28.290018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Tesla T4\n",
      "CUDA is available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# GPU details if available\n",
    "if device.type == \"cuda\":\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(f\"CUDA is available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79b47852",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T22:31:31.217793Z",
     "iopub.status.busy": "2024-12-06T22:31:31.217044Z",
     "iopub.status.idle": "2024-12-06T22:31:31.241938Z",
     "shell.execute_reply": "2024-12-06T22:31:31.241055Z"
    },
    "papermill": {
     "duration": 0.029888,
     "end_time": "2024-12-06T22:31:31.243587",
     "exception": false,
     "start_time": "2024-12-06T22:31:31.213699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monet TFRecord Files: 5\n",
      "Photo TFRecord Files: 20\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "# Define the paths to the dataset\n",
    "GCS_PATH = '/kaggle/input/gan-getting-started'\n",
    "monet_files_path = os.path.join(GCS_PATH, 'monet_tfrec', '*.tfrec')\n",
    "photo_files_path = os.path.join(GCS_PATH, 'photo_tfrec', '*.tfrec')\n",
    "\n",
    "# Load filenames using glob\n",
    "MONET_FILENAMES = glob.glob(monet_files_path)\n",
    "PHOTO_FILENAMES = glob.glob(photo_files_path)\n",
    "\n",
    "print('Monet TFRecord Files:', len(MONET_FILENAMES))\n",
    "print('Photo TFRecord Files:', len(PHOTO_FILENAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b52a57d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T22:31:31.249503Z",
     "iopub.status.busy": "2024-12-06T22:31:31.249268Z",
     "iopub.status.idle": "2024-12-06T22:31:32.728991Z",
     "shell.execute_reply": "2024-12-06T22:31:32.728300Z"
    },
    "papermill": {
     "duration": 1.484904,
     "end_time": "2024-12-06T22:31:32.731011",
     "exception": false,
     "start_time": "2024-12-06T22:31:31.246107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import itertools\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as vutils\n",
    "from torchvision.utils import make_grid\n",
    "import cv2\n",
    "from torchvision.models import vgg16\n",
    "import zipfile\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b18d08a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T22:31:32.737948Z",
     "iopub.status.busy": "2024-12-06T22:31:32.737372Z",
     "iopub.status.idle": "2024-12-06T22:31:32.747528Z",
     "shell.execute_reply": "2024-12-06T22:31:32.746749Z"
    },
    "papermill": {
     "duration": 0.015167,
     "end_time": "2024-12-06T22:31:32.749047",
     "exception": false,
     "start_time": "2024-12-06T22:31:32.733880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 4\n",
    "IMAGE_SIZE = 256\n",
    "LEARNING_RATE = 0.0002\n",
    "BETA1 = 0.5\n",
    "BETA2 = 0.999\n",
    "EPOCHS = 5\n",
    "LAMBDA_CYCLE = 10.0\n",
    "LAMBDA_IDENTITY = 5.0\n",
    "\n",
    "# Visualization class\n",
    "class VisualizeResults:\n",
    "    def __init__(self, save_dir='training_progress4_1'):\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    @staticmethod\n",
    "    def denormalize(tensor):\n",
    "        \"\"\"Convert normalized image tensor back to regular image tensor\"\"\"\n",
    "        tensor = tensor.clone()\n",
    "        tensor = tensor * 0.5 + 0.5\n",
    "        return tensor.clamp(0, 1)\n",
    "    \n",
    "    def save_image_grid(self, real_A, fake_B, real_B, fake_A, epoch, batch_idx):\n",
    "        \"\"\"Save a grid of real and generated images\"\"\"\n",
    "        # Denormalize images\n",
    "        real_A = self.denormalize(real_A)\n",
    "        fake_B = self.denormalize(fake_B)\n",
    "        real_B = self.denormalize(real_B)\n",
    "        fake_A = self.denormalize(fake_A)\n",
    "        \n",
    "        # Create image grid\n",
    "        img_grid = make_grid([\n",
    "            real_A[0], fake_B[0],  # Photo → Monet\n",
    "            real_B[0], fake_A[0],  # Monet → Photo\n",
    "        ], nrow=2)\n",
    "        \n",
    "        # Convert to numpy and transpose\n",
    "        img_grid = img_grid.cpu().numpy().transpose((1, 2, 0))\n",
    "        \n",
    "        # Create figure\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(img_grid)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Add labels\n",
    "        plt.text(32, 20, 'Real Photo', color='white', fontsize=10)\n",
    "        plt.text(IMAGE_SIZE + 32, 20, 'Generated Monet', color='white', fontsize=10)\n",
    "        plt.text(32, IMAGE_SIZE + 20, 'Real Monet', color='white', fontsize=10)\n",
    "        plt.text(IMAGE_SIZE + 32, IMAGE_SIZE + 20, 'Generated Photo', color='white', fontsize=10)\n",
    "        \n",
    "        # Save figure\n",
    "        plt.savefig(f'{self.save_dir}/epoch_{epoch}batch{batch_idx}.png', \n",
    "                   bbox_inches='tight', pad_inches=0.1)\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_losses(self, g_losses, d_losses):\n",
    "        \"\"\"Plot generator and discriminator losses\"\"\"\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(g_losses, label='Generator Loss')\n",
    "        plt.plot(d_losses, label='Discriminator Loss')\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Losses')\n",
    "        plt.legend()\n",
    "        plt.savefig(f'{self.save_dir}/training_losses.png')\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3031cd03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T22:31:32.755062Z",
     "iopub.status.busy": "2024-12-06T22:31:32.754626Z",
     "iopub.status.idle": "2024-12-06T22:31:32.759993Z",
     "shell.execute_reply": "2024-12-06T22:31:32.759220Z"
    },
    "papermill": {
     "duration": 0.009996,
     "end_time": "2024-12-06T22:31:32.761505",
     "exception": false,
     "start_time": "2024-12-06T22:31:32.751509",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataset Class\n",
    "class MonetPhotoDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = glob(os.path.join(root_dir, \"*.jpg\"))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        img_np = np.array(image)\n",
    "        \n",
    "        # Apply histogram equalization to each channel\n",
    "        for i in range(3):\n",
    "            img_np[:,:,i] = cv2.equalizeHist(img_np[:,:,i])\n",
    "            \n",
    "        image = Image.fromarray(img_np)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ec61ea1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T22:31:32.767556Z",
     "iopub.status.busy": "2024-12-06T22:31:32.767302Z",
     "iopub.status.idle": "2024-12-06T22:31:32.778945Z",
     "shell.execute_reply": "2024-12-06T22:31:32.778149Z"
    },
    "papermill": {
     "duration": 0.016436,
     "end_time": "2024-12-06T22:31:32.780446",
     "exception": false,
     "start_time": "2024-12-06T22:31:32.764010",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generator Architecture\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(channels, channels, 3, groups=max(1, channels//4)),  # Grouped convolution for more texture\n",
    "            nn.InstanceNorm2d(channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(0.1),  # Slight dropout for variation\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(channels, channels, 3),\n",
    "            nn.InstanceNorm2d(channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_channels=3, num_residual_blocks=9):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # Initial convolution\n",
    "        model = [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(input_channels, 64, 7),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        \n",
    "        # Downsampling\n",
    "        in_features = 64\n",
    "        out_features = in_features * 2\n",
    "        for _ in range(2):\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features * 2\n",
    "        \n",
    "        # Residual blocks\n",
    "        for _ in range(num_residual_blocks):\n",
    "            model += [ResidualBlock(in_features)]\n",
    "        \n",
    "        # Upsampling\n",
    "        out_features = in_features // 2\n",
    "        for _ in range(2):\n",
    "            model += [\n",
    "                nn.ConvTranspose2d(in_features, out_features, 3, stride=2, padding=1, output_padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features // 2\n",
    "        \n",
    "        # Output layer\n",
    "        model += [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(64, input_channels, 7),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "        \n",
    "        self.model = nn.Sequential(*model)\n",
    "        self.instance_norm = nn.InstanceNorm2d(input_channels, affine=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.instance_norm(x)\n",
    "        return self.model(x)\n",
    "\n",
    "# Discriminator Architecture\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        def discriminator_block(in_filters, out_filters, normalize=True):\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(input_channels, 64, normalize=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, 4, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a56ed0d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T22:31:32.788094Z",
     "iopub.status.busy": "2024-12-06T22:31:32.787274Z",
     "iopub.status.idle": "2024-12-06T22:31:32.805329Z",
     "shell.execute_reply": "2024-12-06T22:31:32.804653Z"
    },
    "papermill": {
     "duration": 0.023148,
     "end_time": "2024-12-06T22:31:32.806849",
     "exception": false,
     "start_time": "2024-12-06T22:31:32.783701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StyleLoss:\n",
    "    def __init__(self):\n",
    "        vgg = vgg16(pretrained=True).features[:16].to(DEVICE)\n",
    "        for param in vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.vgg = vgg\n",
    "    \n",
    "    def compute_style_loss(self, generated, style):\n",
    "        \"\"\"Compute style loss using Gram matrix\"\"\"\n",
    "        def gram_matrix(x):\n",
    "            b, c, h, w = x.size()\n",
    "            features = x.view(b, c, h * w)\n",
    "            G = torch.bmm(features, features.transpose(1, 2))\n",
    "            return G.div(c * h * w)\n",
    "        \n",
    "        # Extract features\n",
    "        gen_features = self.vgg(generated)\n",
    "        style_features = self.vgg(style)\n",
    "        \n",
    "        # Compute Gram matrix style loss\n",
    "        style_gram = gram_matrix(style_features)\n",
    "        gen_gram = gram_matrix(gen_features)\n",
    "        \n",
    "        return nn.functional.mse_loss(gen_gram, style_gram)        \n",
    "\n",
    "# Loss functions\n",
    "class CycleGANLoss:\n",
    "    def __init__(self):\n",
    "        self.mae_loss = nn.L1Loss()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.style_loss = StyleLoss()\n",
    "        self.style_weight = 500.0  # Adjust this to control style transfer intensity\n",
    "\n",
    "\n",
    "    def get_style_transfer_loss(self, generated, style_reference):\n",
    "        \"\"\"Compute additional style transfer loss\"\"\"\n",
    "        return self.style_loss.compute_style_loss(generated, style_reference) * self.style_weight    \n",
    "\n",
    "    def get_color_consistency_loss(self, real, fake):\n",
    "        real_mean = torch.mean(real, dim=[2, 3])\n",
    "        fake_mean = torch.mean(fake, dim=[2, 3])\n",
    "        return self.mae_loss(real_mean, fake_mean)    \n",
    "    \n",
    "    def get_gan_loss(self, pred, target_is_real):\n",
    "        target = torch.ones_like(pred) if target_is_real else torch.zeros_like(pred)\n",
    "        return self.mse_loss(pred, target)\n",
    "    \n",
    "    def get_cycle_loss(self, real, cycled):\n",
    "        return self.mae_loss(real, cycled) * LAMBDA_CYCLE\n",
    "    \n",
    "    def get_identity_loss(self, real, same):\n",
    "        return self.mae_loss(real, same) * LAMBDA_IDENTITY\n",
    "\n",
    "# Training class\n",
    "class CycleGAN:\n",
    "    def __init__(self):\n",
    "        # Initialize generators and discriminators\n",
    "        self.G_AB = Generator().to(DEVICE)\n",
    "        self.G_BA = Generator().to(DEVICE)\n",
    "        self.D_A = Discriminator().to(DEVICE)\n",
    "        self.D_B = Discriminator().to(DEVICE)\n",
    "        \n",
    "        # Initialize optimizers\n",
    "        self.optimizer_G = optim.Adam(\n",
    "            itertools.chain(self.G_AB.parameters(), self.G_BA.parameters()),\n",
    "            lr=LEARNING_RATE, betas=(BETA1, BETA2)\n",
    "        )\n",
    "        self.optimizer_D = optim.Adam(\n",
    "            itertools.chain(self.D_A.parameters(), self.D_B.parameters()),\n",
    "            lr=LEARNING_RATE, betas=(BETA1, BETA2)\n",
    "        )\n",
    "        \n",
    "        # Initialize loss functions\n",
    "        self.criterion = CycleGANLoss()\n",
    "        \n",
    "        # Initialize learning rate schedulers\n",
    "        self.scheduler_G = optim.lr_scheduler.LinearLR(self.optimizer_G, \n",
    "                                                      start_factor=1.0, \n",
    "                                                      end_factor=0.0,\n",
    "                                                      total_iters=EPOCHS)\n",
    "        self.scheduler_D = optim.lr_scheduler.LinearLR(self.optimizer_D,\n",
    "                                                      start_factor=1.0,\n",
    "                                                      end_factor=0.0,\n",
    "                                                      total_iters=EPOCHS)\n",
    "    \n",
    "    def train_step(self, real_A, real_B):\n",
    "        # Generate fake images\n",
    "        fake_B = self.G_AB(real_A)\n",
    "        fake_A = self.G_BA(real_B)\n",
    "        \n",
    "        # Reconstruct images\n",
    "        cycle_A = self.G_BA(fake_B)\n",
    "        cycle_B = self.G_AB(fake_A)\n",
    "        \n",
    "        # Identity mapping\n",
    "        same_A = self.G_BA(real_A)\n",
    "        same_B = self.G_AB(real_B)\n",
    "        \n",
    "        # Train Generators\n",
    "        self.optimizer_G.zero_grad()\n",
    "\n",
    "         # Compute additional style transfer loss\n",
    "        style_loss_AB = self.criterion.get_style_transfer_loss(fake_B, real_B)\n",
    "        style_loss_BA = self.criterion.get_style_transfer_loss(fake_A, real_A)\n",
    "\n",
    "        # Color consistency loss\n",
    "        loss_color = (self.criterion.get_color_consistency_loss(real_A, fake_A) + \n",
    "                     self.criterion.get_color_consistency_loss(real_B, fake_B)) * 5.0\n",
    "        loss_G = loss_color\n",
    "\n",
    "        # Identity loss\n",
    "        loss_identity_A = self.criterion.get_identity_loss(real_A, same_A)\n",
    "        loss_identity_B = self.criterion.get_identity_loss(real_B, same_B)\n",
    "\n",
    "        # GAN loss\n",
    "        loss_GAN_AB = self.criterion.get_gan_loss(self.D_B(fake_B), True)\n",
    "        loss_GAN_BA = self.criterion.get_gan_loss(self.D_A(fake_A), True)\n",
    "\n",
    "        # Cycle loss\n",
    "        loss_cycle_A = self.criterion.get_cycle_loss(real_A, cycle_A)\n",
    "        loss_cycle_B = self.criterion.get_cycle_loss(real_B, cycle_B)\n",
    "\n",
    "         # Combined generator loss - add style transfer loss\n",
    "        loss_G += (loss_identity_A + loss_identity_B + \n",
    "                   loss_GAN_AB + loss_GAN_BA + \n",
    "                   loss_cycle_A + loss_cycle_B + \n",
    "                   style_loss_AB + style_loss_BA)\n",
    "\n",
    "        loss_G.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            itertools.chain(self.G_AB.parameters(), self.G_BA.parameters()), \n",
    "            max_norm=1.0\n",
    "        )\n",
    "        self.optimizer_G.step()\n",
    "\n",
    "        # Train Discriminators\n",
    "        self.optimizer_D.zero_grad()\n",
    "\n",
    "        # Discriminator A loss\n",
    "        loss_D_A_real = self.criterion.get_gan_loss(self.D_A(real_A), True)\n",
    "        loss_D_A_fake = self.criterion.get_gan_loss(self.D_A(fake_A.detach()), False)\n",
    "        loss_D_A = (loss_D_A_real + loss_D_A_fake) * 0.5\n",
    "\n",
    "        # Discriminator B loss\n",
    "        loss_D_B_real = self.criterion.get_gan_loss(self.D_B(real_B), True)\n",
    "        loss_D_B_fake = self.criterion.get_gan_loss(self.D_B(fake_B.detach()), False)\n",
    "        loss_D_B = (loss_D_B_real + loss_D_B_fake) * 0.5\n",
    "\n",
    "        # Combined discriminator loss\n",
    "        loss_D = loss_D_A + loss_D_B\n",
    "\n",
    "        loss_D.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            itertools.chain(self.G_AB.parameters(), self.G_BA.parameters()), \n",
    "            max_norm=1.0\n",
    "        )\n",
    "        self.optimizer_D.step()\n",
    "\n",
    "        return {\n",
    "            'loss_G': loss_G.item(),\n",
    "            'loss_D': loss_D.item(),\n",
    "            'fake_A': fake_A,\n",
    "            'fake_B': fake_B\n",
    "        }\n",
    "    \n",
    "    def save_models(self, epoch):\n",
    "        torch.save(self.G_AB.state_dict(), f'generator_AB_epoch_{epoch}.pth')\n",
    "        torch.save(self.G_BA.state_dict(), f'generator_BA_epoch_{epoch}.pth')\n",
    "        torch.save(self.D_A.state_dict(), f'discriminator_A_epoch_{epoch}.pth')\n",
    "        torch.save(self.D_B.state_dict(), f'discriminator_B_epoch_{epoch}.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e782f94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T22:31:32.813790Z",
     "iopub.status.busy": "2024-12-06T22:31:32.813554Z",
     "iopub.status.idle": "2024-12-06T22:31:32.824384Z",
     "shell.execute_reply": "2024-12-06T22:31:32.823639Z"
    },
    "papermill": {
     "duration": 0.015658,
     "end_time": "2024-12-06T22:31:32.825972",
     "exception": false,
     "start_time": "2024-12-06T22:31:32.810314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Data preprocessing\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n",
    "    ])\n",
    "    \n",
    "    # Load datasets\n",
    "    monet_dataset = MonetPhotoDataset(\"/kaggle/input/gan-getting-started/monet_jpg\", transform=transform)\n",
    "    photo_dataset = MonetPhotoDataset(\"/kaggle/input/gan-getting-started/photo_jpg\", transform=transform)\n",
    "    \n",
    "    monet_loader = DataLoader(monet_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    photo_loader = DataLoader(photo_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    # Initialize model and visualizer\n",
    "    model = CycleGAN()\n",
    "    visualizer = VisualizeResults()\n",
    "    \n",
    "    # Track losses for plotting\n",
    "    g_losses = []\n",
    "    d_losses = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        for i, (monet_imgs, photo_imgs) in enumerate(zip(monet_loader, photo_loader)):\n",
    "            monet_imgs = monet_imgs.to(DEVICE)\n",
    "            photo_imgs = photo_imgs.to(DEVICE)\n",
    "            \n",
    "            # Train step\n",
    "            results = model.train_step(photo_imgs, monet_imgs)\n",
    "            \n",
    "            # Record losses\n",
    "            g_losses.append(results['loss_G'])\n",
    "            d_losses.append(results['loss_D'])\n",
    "            \n",
    "            # Print progress\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Epoch [{epoch}/{EPOCHS}] Batch [{i}/{len(monet_loader)}] \"\n",
    "                      f\"Loss G: {results['loss_G']:.4f}, Loss D: {results['loss_D']:.4f}\")\n",
    "            \n",
    "            # Save images for visualization every 100th batch\n",
    "            if i % 100 == 0:\n",
    "                visualizer.save_image_grid(photo_imgs, results['fake_B'], \n",
    "                                         monet_imgs, results['fake_A'], \n",
    "                                         epoch, i)\n",
    "        \n",
    "        # Save model checkpoints at regular intervals\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            model.save_models(epoch + 1)\n",
    "        \n",
    "        # Plot and save losses\n",
    "        visualizer.plot_losses(g_losses, d_losses)\n",
    "    \n",
    "    print(\"Training completed.\")\n",
    "\n",
    "def generate_images(num_images=7100):\n",
    "    # Load trained generator\n",
    "    generator = Generator().to(DEVICE)\n",
    "    generator.load_state_dict(torch.load('generator_AB_epoch_5.pth'))  # Load the last saved model\n",
    "    generator.eval()\n",
    "    \n",
    "    # Create a zip file to store the images\n",
    "    zip_filename = '/kaggle/working/images.zip'\n",
    "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    \n",
    "        # Data preprocessing\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        \n",
    "        # Load photo dataset\n",
    "        photo_dataset = MonetPhotoDataset(\"/kaggle/input/gan-getting-started/photo_jpg\", transform=transform)\n",
    "        photo_loader = DataLoader(photo_dataset, batch_size=1, shuffle=False)\n",
    "        \n",
    "        # Generate images\n",
    "        with torch.no_grad():\n",
    "            for i, photo in enumerate(itertools.islice(photo_loader, num_images)):\n",
    "                photo = photo.to(DEVICE)\n",
    "                fake_monet = generator(photo)\n",
    "                \n",
    "                # Save individual generated image to the zip file\n",
    "                fake_monet_img = VisualizeResults.denormalize(fake_monet)  # Use as a static method\n",
    "                fake_monet_img = fake_monet_img.cpu().squeeze(0).permute(1, 2, 0).numpy()\n",
    "                generated_image = Image.fromarray((fake_monet_img * 255).astype(np.uint8))\n",
    "                with zipf.open(f'generated_{i}.jpg', 'w') as imgf:\n",
    "                    generated_image.save(imgf, format='JPEG')\n",
    "\n",
    "                # Explicitly clear tensors and collect garbage to free memory\n",
    "                del photo, fake_monet, fake_monet_img\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                if i % 100 == 0:\n",
    "                    print(f\"Generated {i} images\")\n",
    "                \n",
    "    print(f\"All images have been saved to {zip_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94028663",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T22:31:32.832112Z",
     "iopub.status.busy": "2024-12-06T22:31:32.831471Z",
     "iopub.status.idle": "2024-12-06T23:02:53.693763Z",
     "shell.execute_reply": "2024-12-06T23:02:53.692882Z"
    },
    "papermill": {
     "duration": 1880.877257,
     "end_time": "2024-12-06T23:02:53.705664",
     "exception": false,
     "start_time": "2024-12-06T22:31:32.828407",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
      "100%|██████████| 528M/528M [00:02<00:00, 213MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/5] Batch [0/75] Loss G: 21.9818, Loss D: 1.2837\n",
      "Epoch [1/5] Batch [0/75] Loss G: 6.2680, Loss D: 0.4404\n",
      "Epoch [2/5] Batch [0/75] Loss G: 8.2299, Loss D: 0.5152\n",
      "Epoch [3/5] Batch [0/75] Loss G: 5.4436, Loss D: 0.4353\n",
      "Epoch [4/5] Batch [0/75] Loss G: 6.2761, Loss D: 0.3710\n",
      "Training completed.\n",
      "Training completed!\n",
      "Generating Monet-style images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/766517978.py:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  generator.load_state_dict(torch.load('generator_AB_epoch_5.pth'))  # Load the last saved model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 0 images\n",
      "Generated 100 images\n",
      "Generated 200 images\n",
      "Generated 300 images\n",
      "Generated 400 images\n",
      "Generated 500 images\n",
      "Generated 600 images\n",
      "Generated 700 images\n",
      "Generated 800 images\n",
      "Generated 900 images\n",
      "Generated 1000 images\n",
      "Generated 1100 images\n",
      "Generated 1200 images\n",
      "Generated 1300 images\n",
      "Generated 1400 images\n",
      "Generated 1500 images\n",
      "Generated 1600 images\n",
      "Generated 1700 images\n",
      "Generated 1800 images\n",
      "Generated 1900 images\n",
      "Generated 2000 images\n",
      "Generated 2100 images\n",
      "Generated 2200 images\n",
      "Generated 2300 images\n",
      "Generated 2400 images\n",
      "Generated 2500 images\n",
      "Generated 2600 images\n",
      "Generated 2700 images\n",
      "Generated 2800 images\n",
      "Generated 2900 images\n",
      "Generated 3000 images\n",
      "Generated 3100 images\n",
      "Generated 3200 images\n",
      "Generated 3300 images\n",
      "Generated 3400 images\n",
      "Generated 3500 images\n",
      "Generated 3600 images\n",
      "Generated 3700 images\n",
      "Generated 3800 images\n",
      "Generated 3900 images\n",
      "Generated 4000 images\n",
      "Generated 4100 images\n",
      "Generated 4200 images\n",
      "Generated 4300 images\n",
      "Generated 4400 images\n",
      "Generated 4500 images\n",
      "Generated 4600 images\n",
      "Generated 4700 images\n",
      "Generated 4800 images\n",
      "Generated 4900 images\n",
      "Generated 5000 images\n",
      "Generated 5100 images\n",
      "Generated 5200 images\n",
      "Generated 5300 images\n",
      "Generated 5400 images\n",
      "Generated 5500 images\n",
      "Generated 5600 images\n",
      "Generated 5700 images\n",
      "Generated 5800 images\n",
      "Generated 5900 images\n",
      "Generated 6000 images\n",
      "Generated 6100 images\n",
      "Generated 6200 images\n",
      "Generated 6300 images\n",
      "Generated 6400 images\n",
      "Generated 6500 images\n",
      "Generated 6600 images\n",
      "Generated 6700 images\n",
      "Generated 6800 images\n",
      "Generated 6900 images\n",
      "Generated 7000 images\n",
      "All images have been saved to /kaggle/working/images.zip\n",
      "Image generation completed!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Starting training...\")\n",
    "    train()\n",
    "    print(\"Training completed!\")\n",
    "    \n",
    "    # Generate images\n",
    "    print(\"Generating Monet-style images...\")\n",
    "    generate_images()\n",
    "    print(\"Image generation completed!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 1475600,
     "sourceId": 21755,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1888.989642,
   "end_time": "2024-12-06T23:02:54.935532",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-06T22:31:25.945890",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
